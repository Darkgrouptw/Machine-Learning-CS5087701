{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spooky Author Identification\n",
    "[連結](https://www.kaggle.com/c/spooky-author-identification/submissions?sortBy=date&group=all&page=1)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 為了讓設定是可以 重複製造\n",
    "random.seed(1)\n",
    "tf.set_random_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 字典檔\n",
    "WordDictionary = []\n",
    "PeopleDictionary = [\"EAP\", \"HPL\", \"MWS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把垃圾字元去掉\n",
    "def deleteNotWord(line):\n",
    "    line = line.replace(\",\", \"\")\n",
    "    line = line.replace(\";\", \"\")\n",
    "    line = line.replace(\"\\'\", \"\")\n",
    "    line = line.replace(\"\\\"\", \"\")\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根據單字，找在字典裏面的哪裡\n",
    "def findIndexInWordDictionary(word):\n",
    "    word = word.lower()\n",
    "    for i in range(0, len(WordDictionary)):\n",
    "        # 如果有找到的話\n",
    "        if(word == WordDictionary[i]):\n",
    "            return i\n",
    "        \n",
    "    # 沒找到\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找 People Dictionary 裡面，名字在哪個 index\n",
    "def findIndexInPeopleDictionary(name):\n",
    "    outputArray = [0] * len(PeopleDictionary)\n",
    "    for i in range(len(PeopleDictionary)):\n",
    "        if(PeopleDictionary[i] == name):\n",
    "            outputArray[i] = 1\n",
    "    return outputArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def passAndAddWord(line):\n",
    "    words = line.split(\" \")\n",
    "    for i in range(0, len(words)):\n",
    "        if(findIndexInWordDictionary(words[i]) == -1):\n",
    "            WordDictionary.append(words[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 讀檔\n",
    "trainData = open(\"./Data/train.csv\", \"r\")\n",
    "\n",
    "# 要存出去的部分\n",
    "TextList = []\n",
    "AuthorList = []\n",
    "\n",
    "# 吐掉第一行\n",
    "trainData.readline()\n",
    "\n",
    "# 將檔案做處理\n",
    "for line in trainData:\n",
    "    # 依照 \",\" 分，並把最前面的雙引號及後面去掉\n",
    "    lineData = line.split(\"\\\",\\\"\")\n",
    "    lineData[0] = lineData[0][1:]\n",
    "    lineData[2] = lineData[2][0:len(lineData[2]) - 2]\n",
    "    \n",
    "    lineData[1] = deleteNotWord(lineData[1])\n",
    "    passAndAddWord(lineData[1])\n",
    "    \n",
    "    # 加進 List 供以後存取\n",
    "    TextList.append(lineData[1])\n",
    "    AuthorList.append(lineData[2])\n",
    "\n",
    "# 測試字典檔大小\n",
    "DictionarySize = len(WordDictionary)\n",
    "print(DictionarySize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 關閉檔案\n",
    "trainData.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalFileSize = 19579\n",
    "testFileSize = int(totalFileSize * 0.01)\n",
    "trainFileSize = totalFileSize - testFileSize\n",
    "print(\"Total File Size: \" + format(totalFileSize))\n",
    "print(\"Train File Size: \" + format(trainFileSize))\n",
    "print(\"Test File Size: \" + format(testFileSize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取出 Test 的 Index\n",
    "TestIndex = random.sample(range(totalFileSize), testFileSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 判斷是不是在 Test Index 裡面\n",
    "def findIndexInTestIndex(index):\n",
    "    for i in range(0, len(TestIndex)):\n",
    "        if(TestIndex[i] == index):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 判斷字在不在字典裏面，並 Output 有的數目\n",
    "def CountWordInDictionary(line):\n",
    "    outputArray = [0] * DictionarySize\n",
    "    \n",
    "    lineData = line.split(\" \")\n",
    "    for word in lineData:\n",
    "        index = findIndexInWordDictionary(word)\n",
    "        outputArray[index] += 1\n",
    "    \n",
    "    wordCount = len(lineData)\n",
    "    for i in range(0, DictionarySize):\n",
    "        outputArray[i] /= wordCount\n",
    "    return outputArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 要丟進 Tensorflow 學習的參數\n",
    "# Train\n",
    "TrainInputList = []\n",
    "TrainOutputList = []\n",
    "# Test\n",
    "TestInputList = []\n",
    "TestOutputList = []\n",
    "for i in range(0, len(TextList)):\n",
    "    # 先處理完第 i 筆資料\n",
    "    TempInput = CountWordInDictionary(TextList[i])\n",
    "    TempOutput = findIndexInPeopleDictionary(AuthorList[i])\n",
    "    \n",
    "    # 判斷要加到哪裡\n",
    "    if(not findIndexInTestIndex(i)):\n",
    "        TrainInputList.append(TempInput)\n",
    "        TrainOutputList.append(TempOutput)\n",
    "    else:\n",
    "        TestInputList.append(TempInput)\n",
    "        TestOutputList.append(TempOutput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 開始建構 Tensorflow\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1_size = 10000\n",
    "layer2_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 輸入\n",
    "inputWordProb = tf.placeholder(tf.float32, [None, len(WordDictionary)], name = \"InputProb\")\n",
    "\n",
    "# 輸出\n",
    "outputProb = tf.placeholder(tf.float32,[None, len(PeopleDictionary)], name = \"OutputProb\")\n",
    "  \n",
    "# 初始化\n",
    "weghtInit = tf.random_normal_initializer(mean = 0, stddev= = 0.3)\n",
    "biasInit = tf.random_normal_initializer(mean = 0, stddev = 0.1)\n",
    "\n",
    "# Layer 1\n",
    "with tf.name_scope(\"layer1\"):\n",
    "    layer1 = tf.layers.dense(\n",
    "        inputs = inputWordProb,\n",
    "        units = layer1_size,\n",
    "        activation = tf.nn.relu,\n",
    "        kernel_initializer = weghtInit,\n",
    "        bias_initializer = bias_initializer\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
