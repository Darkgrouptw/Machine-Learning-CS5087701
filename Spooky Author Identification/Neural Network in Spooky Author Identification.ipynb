{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spooky Author Identification\n",
    "[連結](https://www.kaggle.com/c/spooky-author-identification/submissions?sortBy=date&group=all&page=1)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 為了讓設定是可以 重複製造\n",
    "random.seed(1)\n",
    "tf.set_random_seed(1)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 字典檔\n",
    "WordDictionary = []\n",
    "PeopleDictionary = [\"EAP\", \"HPL\", \"MWS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把垃圾字元去掉\n",
    "def deleteNotWord(line):\n",
    "    line = line.replace(\",\", \"\")\n",
    "    line = line.replace(\";\", \"\")\n",
    "    line = line.replace(\":\", \"\")\n",
    "    line = line.replace(\".\", \"\")\n",
    "    line = line.replace(\"?\", \"\")\n",
    "    line = line.replace(\"\\'\", \"\")\n",
    "    line = line.replace(\"\\\"\", \"\")\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根據單字，找在字典裏面的哪裡\n",
    "def findIndexInWordDictionary(word):\n",
    "    word = word.lower()\n",
    "    for i in range(0, len(WordDictionary)):\n",
    "        # 如果有找到的話\n",
    "        if(word == WordDictionary[i]):\n",
    "            return i\n",
    "        \n",
    "    # 沒找到\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找 People Dictionary 裡面，名字在哪個 index\n",
    "def findIndexInPeopleDictionary(name):\n",
    "    outputArray = [0] * len(PeopleDictionary)\n",
    "    for i in range(len(PeopleDictionary)):\n",
    "        if(PeopleDictionary[i] == name):\n",
    "            outputArray[i] = 1\n",
    "    return outputArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def passAndAddWord(line):\n",
    "    words = line.split(\" \")\n",
    "    for i in range(0, len(words)):\n",
    "        if(findIndexInWordDictionary(words[i]) == -1):\n",
    "            WordDictionary.append(words[i].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25412\n",
      "agir\n"
     ]
    }
   ],
   "source": [
    "# 讀檔\n",
    "trainData = open(\"./Data/train.csv\", \"r\", encoding=\"utf-8\")\n",
    "trainDataList = trainData.read().splitlines()\n",
    "\n",
    "# 略調第一行\n",
    "trainDataList = trainDataList[1: len(trainDataList)]\n",
    "\n",
    "# 要存出去的部分\n",
    "TextList = []\n",
    "AuthorList = []\n",
    "\n",
    "# 判斷有沒有 Dictionary 檔，先讀進來\n",
    "IsDictionaryExists = os.path.exists(\"./dictionary.txt\")\n",
    "if(IsDictionaryExists):\n",
    "    dictionaryFile = open(\"./dictionary.txt\", \"r\", encoding=\"utf-8\")\n",
    "    tempDictionaryList = dictionaryFile.read().splitlines()\n",
    "        \n",
    "    # 把每一個讀進來\n",
    "    for line in tempDictionaryList:\n",
    "        WordDictionary.append(line)\n",
    "    dictionaryFile.close()\n",
    "    \n",
    "\n",
    "# 將檔案做處理\n",
    "for line in trainDataList:\n",
    "    # 依照 \",\" 分，並把最前面的雙引號及後面去掉\n",
    "    lineData = line.split(\"\\\",\\\"\")\n",
    "    lineData[0] = lineData[0][1:]\n",
    "    lineData[2] = lineData[2][0:len(lineData[2]) - 1]\n",
    "    \n",
    "    lineData[1] = deleteNotWord(lineData[1])\n",
    "    \n",
    "    if(not IsDictionaryExists):\n",
    "        passAndAddWord(lineData[1])\n",
    "    \n",
    "    # 加進 List 供以後存取\n",
    "    TextList.append(lineData[1])\n",
    "    AuthorList.append(lineData[2])\n",
    "    \n",
    "# 讀完資料之後要經過處理，因為怕每次都處理很久，所以存檔起來\n",
    "if(not IsDictionaryExists):        \n",
    "    dictionaryFile = open(\"./dictionary.txt\", \"w\", encoding=\"utf-8\")\n",
    "    \n",
    "    # 寫檔\n",
    "    for i in range(0, len(WordDictionary)):\n",
    "        dictionaryFile.write(WordDictionary[i] + \"\\n\")\n",
    "    dictionaryFile.close()\n",
    "    \n",
    "# 測試字典檔大小\n",
    "DictionarySize = len(WordDictionary)\n",
    "print(DictionarySize)\n",
    "print(WordDictionary[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 關閉檔案\n",
    "trainData.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total File Size: 19579\n",
      "Train File Size: 19384\n",
      "Test File Size: 195\n"
     ]
    }
   ],
   "source": [
    "totalFileSize = 19579\n",
    "testFileSize = int(totalFileSize * 0.01)\n",
    "trainFileSize = totalFileSize - testFileSize\n",
    "print(\"Total File Size: \" + format(totalFileSize))\n",
    "print(\"Train File Size: \" + format(trainFileSize))\n",
    "print(\"Test File Size: \" + format(testFileSize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取出 Test 的 Index\n",
    "TestIndex = random.sample(range(totalFileSize), testFileSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 判斷是不是在 Test Index 裡面\n",
    "def findIndexInTestIndex(index):\n",
    "    for i in range(0, len(TestIndex)):\n",
    "        if(TestIndex[i] == index):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 判斷字在不在字典裏面，並 Output 有的數目\n",
    "def CountWordInDictionary(line):\n",
    "    outputArray = [0] * DictionarySize\n",
    "    \n",
    "    lineData = line.split(\" \")\n",
    "    \n",
    "    # 把在裡面的值 ++\n",
    "    countInDictionary = 0\n",
    "    for word in lineData:\n",
    "        index = findIndexInWordDictionary(word)\n",
    "        if(index != -1):\n",
    "            outputArray[index] += 1\n",
    "            countInDictionary += 1\n",
    "    \n",
    "    for i in range(0, DictionarySize):\n",
    "        outputArray[i] /= countInDictionary\n",
    "    return outputArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 要丟進 Tensorflow 學習的參數\n",
    "# Train\n",
    "TrainInputList = []\n",
    "TrainOutputList = []\n",
    "# Test\n",
    "TestInputList = []\n",
    "TestOutputList = []\n",
    "\n",
    "# 假設之前沒有做過的話\n",
    "for i in range(0, len(TextList)):\n",
    "    # 先處理完第 i 筆資料\n",
    "    TempInput = CountWordInDictionary(TextList[i])\n",
    "    TempOutput = findIndexInPeopleDictionary(AuthorList[i])\n",
    "    \n",
    "    # 判斷要加到哪裡\n",
    "    if(not findIndexInTestIndex(i)):\n",
    "        TrainInputList.append(TempInput)\n",
    "        TrainOutputList.append(TempOutput)\n",
    "    else:\n",
    "        TestInputList.append(TempInput)\n",
    "        TestOutputList.append(TempOutput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 開始建構 Tensorflow\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1_size = 100\n",
    "layer2_size = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 輸入\n",
    "inputWordProb = tf.placeholder(tf.float32, [None, len(WordDictionary)], name = \"InputProb\")\n",
    "\n",
    "# 輸出\n",
    "labelProb = tf.placeholder(tf.float32,[None, len(PeopleDictionary)], name = \"LabelProb\")\n",
    "  \n",
    "# 初始化\n",
    "weghtInit = tf.random_normal_initializer(mean = 0, stddev = 0.3)\n",
    "biasInit = tf.random_normal_initializer(mean = 0, stddev = 0.1)\n",
    "\n",
    "# Layer 1\n",
    "layer1 = tf.layers.dense(\n",
    "    inputs = inputWordProb,\n",
    "    units = layer1_size,\n",
    "    activation = tf.nn.relu,\n",
    "    kernel_initializer = weghtInit,\n",
    "    bias_initializer = biasInit,\n",
    "    name = \"Layer1\"\n",
    ")\n",
    "\n",
    "# Layer 2\n",
    "layer2 = tf.layers.dense(\n",
    "    inputs = layer1,\n",
    "    units = layer1_size,\n",
    "    activation = tf.nn.relu,\n",
    "    kernel_initializer = weghtInit,\n",
    "    bias_initializer = biasInit,\n",
    "    name = \"Layer2\"\n",
    ")\n",
    "\n",
    "    # Output Layer\n",
    "layer3 = tf.layers.dense(\n",
    "    inputs = layer2,\n",
    "    units = len(PeopleDictionary),\n",
    "    kernel_initializer = weghtInit,\n",
    "    bias_initializer = biasInit,\n",
    "    name = \"Layer3\"\n",
    ")\n",
    "\n",
    "with tf.name_scope(\"OutputProb\"):\n",
    "    # 最後輸出轉成機率\n",
    "    outputProb = tf.nn.softmax(layer3, name = \"OutputProb\")\n",
    "    predictIndex = tf.argmax(outputProb, 1)\n",
    "    trueIndex = tf.argmax(labelProb, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost\n",
    "with tf.name_scope(\"Cost\"):\n",
    "    cost = tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits = outputProb,\n",
    "        labels = labelProb,\n",
    "        name = \"Cost\"\n",
    "    )\n",
    "    cost_mean = tf.reduce_mean(cost)\n",
    "    tf.summary.scalar(\"Cost\", cost_mean)\n",
    "\n",
    "# Optimize\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = 1e-2 ).minimize(cost_mean)\n",
    "\n",
    "# 算準確率\n",
    "with tf.name_scope(\"Accuary\"):\n",
    "    correctList = tf.equal(predictIndex, trueIndex)              # 出來會是一個 boolean 的陣列\n",
    "    accuracyRate = tf.reduce_mean(tf.cast(correctList, tf.float32))\n",
    "    \n",
    "    tf.summary.scalar(\"Accuary\", accuracyRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立 Session\n",
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())\n",
    "logs = tf.summary.FileWriter(\"./logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs.add_graph(session.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 100\n",
    "def batchData():\n",
    "    # 隨機拿 n 筆資料\n",
    "    indexList = np.random.choice(len(TrainInputList), batchSize)\n",
    "    \n",
    "    textList = []\n",
    "    authorList = []\n",
    "    for i in range(0, batchSize):\n",
    "        textList.append(TrainInputList[indexList[i]])\n",
    "        authorList.append(TrainOutputList[indexList[i]])\n",
    "        \n",
    "    return textList, authorList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalIter = 0\n",
    "def train(num_iter):\n",
    "    global totalIter\n",
    "    \n",
    "    for i in range(0, num_iter):\n",
    "        inputData, outputData = batchData()\n",
    "        \n",
    "        feedData = {inputWordProb: inputData, labelProb: outputData}\n",
    "        session.run(optimizer, feed_dict = feedData)\n",
    "        \n",
    "        # 每 5 次寫進 Tensorboard 裡\n",
    "        if(i % 5 == 0):\n",
    "            merge = tf.summary.merge_all()\n",
    "            result = session.run(merge, feed_dict = feedData)\n",
    "            logs.add_summary(result, i + totalIter)\n",
    "            \n",
    "    totalIter += num_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練 1k 次\n",
    "train(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_accuracy():\n",
    "    feedData = {inputWordProb: TestInputList, labelProb: TestOutputList}\n",
    "    acc = session.run(accuracyRate, feed_dict= feedData)\n",
    "    \n",
    "    # 算準確度\n",
    "    print(\"Accuracy: \" + format(acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 56.41025900840759\n"
     ]
    }
   ],
   "source": [
    "print_accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 輸出 Test 檔案\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testFile = open(\"./Data/test.csv\", \"r\", encoding = \"utf-8\")\n",
    "tempTestFile = testFile.read().splitlines()\n",
    "tempTestFile = tempTestFile[1:]                                    # 把第一行垃圾訊息去掉\n",
    "\n",
    "testOutputFile = open(\"./Data/test_pred.csv\", \"w\", encoding = \"utf-8\")\n",
    "testOutputFile.write(\"\\\"id\\\",\\\"EAP\\\",\\\"HPL\\\",\\\"MWS\\\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 處理資料，並預測\n",
    "testData = []\n",
    "idList = []\n",
    "for line in tempTestFile:\n",
    "    # 依照 \",\" 分，並把最前面的雙引號及後面去掉\n",
    "    lineData = line.split(\"\\\",\\\"\")\n",
    "    lineData[0] = lineData[0][1:]\n",
    "    lineData[1] = deleteNotWord(lineData[1])\n",
    "    \n",
    "    idList.append(lineData[0])\n",
    "    testData.append(CountWordInDictionary(lineData[1]))\n",
    "    \n",
    "# 丟進去跑結過\n",
    "feedData = {inputWordProb: testData}\n",
    "predResult = session.run(outputProb, feed_dict=feedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 寫出檔案\n",
    "for i in range(0, len(predResult)):\n",
    "    testOutputFile.write(\"\\\"\" +  idList[i] + \"\\\",\" + \n",
    "                         format(predResult[i][0]) + \",\" + \n",
    "                         format(predResult[i][1]) + \",\" + \n",
    "                         format(predResult[i][2]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "testFile.close()\n",
    "testOutputFile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
